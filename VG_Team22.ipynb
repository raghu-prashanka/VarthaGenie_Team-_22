{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Q4nxbFacNpIO"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.te import Telugu\n",
        "nlp = Telugu()  # use directly\n",
        "nlp = spacy.blank(\"te\")\n",
        "# Pkgs for Normalizing Text\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation\n",
        "# Import Heapq for Finding the Top N Sentences\n",
        "from heapq import nlargest\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    docx = nlp(raw_text)\n",
        "    stopwords = list(STOP_WORDS)\n",
        "    # Build Word Frequency # word.text is tokenization in spacy\n",
        "    word_frequencies = {}\n",
        "    for word in docx:\n",
        "        if word.text not in stopwords:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "\n",
        "\n",
        "    maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
        "    # Sentence Tokens\n",
        "    sentences=sentence_tokenize.sentence_split(docx, lang='tel')\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    #create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "    vector=c.transform(sentences)\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    normalized_matrix = TfidfTransformer().fit_transform(bow_matrix)\n",
        "    similarity_graph = normalized_matrix * normalized_matrix.T\n",
        "    import networkx as nx\n",
        "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    sentence_array = sorted(((scores[i],s) for i,s in enumerate(sentences)),reverse=True)\n",
        "    freq_max =float(sentence_array[0][0])\n",
        "    freq_min=float(sentence_array[len(sentence_array) - 1][0])\n",
        "    temp_array = []\n",
        "    for i in range(0,len(sentence_array)):\n",
        "        if freq_max - freq_min == 0:\n",
        "            temp_array.append(0)\n",
        "        else:\n",
        "            temp_array.append((float(sentence_array[i][0]) - freq_min)/(freq_max - freq_min))\n",
        "            threshold = (sum(temp_array) /len(temp_array)) + 0.25\n",
        "            seq_list = []\n",
        "            for i in range(0,len(temp_array)):\n",
        "                if temp_array[i] > threshold:\n",
        "                        sentence_list.append(sentence_array[i][1])\n",
        "                        for sentence in sentences:\n",
        "                            if sentence in sentence_list:\n",
        "                                seq_list.append(sentence)\n",
        "                                summary = seq_list\n",
        "                                return summary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install indicnlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lChil1vXNql9",
        "outputId": "110a96f4-717e-4a02-912a-3c70798c6900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indicnlp\n",
            "  Downloading indicnlp-0.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: indicnlp\n",
            "Successfully installed indicnlp-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "6Y9Q7VuERmc6",
        "outputId": "fa4a090d-b35a-450a-d2ab-d8b5582e1938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'spacy.tokens.doc.Doc' object has no attribute 'strip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-84d129729e9c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Call Text_summarizer function with raw text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-05a5edca8fd1>\u001b[0m in \u001b[0;36mText_summarizer\u001b[0;34m(raw_docx)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mword_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword_frequencies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmaximum_frequncy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Sentence Tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentence_tokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m#create vectorizer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/indicnlp/tokenize/sentence_tokenize.py\u001b[0m in \u001b[0;36msentence_split\u001b[0;34m(text, lang, delim_pat)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mcand_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdelim_pat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mp1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'spacy.tokens.doc.Doc' object has no attribute 'strip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    stopwords = []  # No need for stopwords in Telugu\n",
        "    # Build Word Frequency # word.text is tokenization in spacy\n",
        "    word_frequencies = {}\n",
        "    sentences = sentence_tokenize.sentence_split(raw_text, lang='te')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    # Sentence Tokens\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    # create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "    vector = c.transform(sentences)\n",
        "    from sklearn.feature_extraction.text import TfidfTransformer\n",
        "    normalized_matrix = TfidfTransformer().fit_transform(bow_matrix)\n",
        "    similarity_graph = normalized_matrix * normalized_matrix.T\n",
        "    import networkx as nx\n",
        "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "    sentence_array = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "    freq_max = float(sentence_array[0][0])\n",
        "    freq_min = float(sentence_array[len(sentence_array) - 1][0])\n",
        "    temp_array = []\n",
        "    sentence_list = []\n",
        "    for i in range(0, len(sentence_array)):\n",
        "        if freq_max - freq_min == 0:\n",
        "            temp_array.append(0)\n",
        "        else:\n",
        "            temp_array.append((float(sentence_array[i][0]) - freq_min) / (freq_max - freq_min))\n",
        "    threshold = (sum(temp_array) / len(temp_array)) + 0.25\n",
        "    seq_list = []\n",
        "    for i in range(0, len(temp_array)):\n",
        "        if temp_array[i] > threshold:\n",
        "            sentence_list.append(sentence_array[i][1])\n",
        "    summary = sentence_list\n",
        "    return summary\n",
        "\n",
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "9Zj7pPCPR_hD",
        "outputId": "bc7f7149-9be3-4a41-ed29-d9901d618452"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'networkx' has no attribute 'from_scipy_sparse_matrix'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-828202762aad>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Call Text_summarizer function with raw text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-828202762aad>\u001b[0m in \u001b[0;36mText_summarizer\u001b[0;34m(raw_docx)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0msimilarity_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalized_matrix\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnormalized_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mnx_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_scipy_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0msentence_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_scipy_sparse_matrix'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.te import Telugu\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "import networkx as nx\n",
        "\n",
        "nlp = Telugu()\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    docx = nlp(raw_text)\n",
        "    stopwords = []  # No need for stopwords in Telugu\n",
        "\n",
        "    word_frequencies = {}\n",
        "    sentences = sentence_tokenize.sentence_split(raw_text, lang='te')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    # Create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "\n",
        "    # Transform count matrix to TF-IDF representation\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    normalized_matrix = tfidf_transformer.fit_transform(bow_matrix)\n",
        "\n",
        "    # Convert to CSR format\n",
        "    normalized_csr_matrix = csr_matrix(normalized_matrix)\n",
        "\n",
        "    # Calculate similarity graph\n",
        "    similarity_graph = normalized_csr_matrix * normalized_csr_matrix.T\n",
        "\n",
        "    # Create graph from the similarity matrix\n",
        "    nx_graph = nx.from_scipy_sparse_matrix(similarity_graph)\n",
        "\n",
        "    # Calculate pagerank scores\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # Sort sentences by score\n",
        "    sentence_array = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = [sentence[1] for sentence in sentence_array[:3]]  # Adjust number of sentences in summary as needed\n",
        "    return summary\n",
        "\n",
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "Rf5Kmt3ASUs9",
        "outputId": "53a99c96-083d-44cc-8cbe-29963d207438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'networkx' has no attribute 'from_scipy_sparse_matrix'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-39aa249e6a1e>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Call Text_summarizer function with raw text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText_summarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-39aa249e6a1e>\u001b[0m in \u001b[0;36mText_summarizer\u001b[0;34m(raw_docx)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Create graph from the similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mnx_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_scipy_sparse_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# Calculate pagerank scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'from_scipy_sparse_matrix'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade networkx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yzv6h5S3SGIq",
        "outputId": "bd4149ae-b885-4efc-bf3b-2505860340e6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade indic-nlp-library\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "ifgjzWkJOMAt",
        "outputId": "8c6773be-9663-4ecd-d1dc-2023324264e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m759.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library) (2024.1)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.16.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (24.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n",
            "Installing collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "indicnlp"
                ]
              },
              "id": "ea775ac4d0fc47d68d520d1f227b7306"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your Spacy Doc object\n",
        "doc = nlp(raw_text)\n",
        "\n",
        "# Convert Spacy Doc object to string\n",
        "raw_text = doc.text\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "rMQzWon6RwjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.te import Telugu\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "import networkx as nx\n",
        "\n",
        "nlp = Telugu()\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    docx = nlp(raw_text)\n",
        "    stopwords = []  # No need for stopwords in Telugu\n",
        "\n",
        "    word_frequencies = {}\n",
        "    sentences = sentence_tokenize.sentence_split(raw_text, lang='te')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    # Create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "\n",
        "    # Transform count matrix to TF-IDF representation\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    normalized_matrix = tfidf_transformer.fit_transform(bow_matrix)\n",
        "\n",
        "    # Convert to CSR format\n",
        "    normalized_csr_matrix = csr_matrix(normalized_matrix)\n",
        "\n",
        "    # Create networkx graph\n",
        "    nx_graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        nx_graph.add_node(i, sentence=sentence)\n",
        "\n",
        "    # Loop through similarity matrix and add edges\n",
        "    for i in range(normalized_csr_matrix.shape[0]):\n",
        "        for j in range(normalized_csr_matrix.shape[1]):\n",
        "            weight = normalized_csr_matrix[i, j]\n",
        "            if weight > 0:\n",
        "                nx_graph.add_edge(i, j, weight=weight)\n",
        "\n",
        "    # Calculate pagerank scores\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # Sort sentences by score\n",
        "    sorted_sentences = sorted(scores, key=scores.get, reverse=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = [nx_graph.nodes[i]['sentence'] for i in sorted_sentences[:3]]  # Adjust number of sentences in summary as needed\n",
        "    return summary\n",
        "\n",
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HiquCrKShxo",
        "outputId": "9ec3fe47-45a0-454e-fd9a-74a966c33d0f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్\\u200cల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి.', 'ఇది ఆంధ్రప్రదేశ్\\u200cలో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది.', 'తదితర వివరాలకు వాట్సాప్\\u200cకు రిజిస్టర్ అవ్వండి: 9701146246']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.te import Telugu\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "import networkx as nx\n",
        "\n",
        "nlp = Telugu()\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    docx = nlp(raw_text)\n",
        "    stopwords = []  # No need for stopwords in Telugu\n",
        "\n",
        "    word_frequencies = {}\n",
        "    sentences = sentence_tokenize.sentence_split(raw_text, lang='te')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    # Create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "\n",
        "    # Transform count matrix to TF-IDF representation\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    normalized_matrix = tfidf_transformer.fit_transform(bow_matrix)\n",
        "\n",
        "    # Convert to CSR format\n",
        "    normalized_csr_matrix = csr_matrix(normalized_matrix)\n",
        "\n",
        "    # Create networkx graph\n",
        "    nx_graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        nx_graph.add_node(i, sentence=sentence)\n",
        "\n",
        "    # Loop through similarity matrix and add edges\n",
        "    for i in range(normalized_csr_matrix.shape[0]):\n",
        "        for j in range(normalized_csr_matrix.shape[1]):\n",
        "            weight = normalized_csr_matrix[i, j]\n",
        "            if weight > 0:\n",
        "                nx_graph.add_edge(i, j, weight=weight)\n",
        "\n",
        "    # Calculate pagerank scores\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # Sort sentences by score\n",
        "    sorted_sentences = sorted(scores, key=scores.get, reverse=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = [sentences[i] for i in sorted_sentences[:3]]  # Adjust number of sentences in summary as needed\n",
        "    return summary\n",
        "\n",
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhJFhNeESsB4",
        "outputId": "dbc27bd3-d66a-4e69-88e7-f7a5f472f55a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్\\u200cల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి.', 'ఇది ఆంధ్రప్రదేశ్\\u200cలో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది.', 'తదితర వివరాలకు వాట్సాప్\\u200cకు రిజిస్టర్ అవ్వండి: 9701146246']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.te import Telugu\n",
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from scipy.sparse import csr_matrix\n",
        "import networkx as nx\n",
        "\n",
        "nlp = Telugu()\n",
        "\n",
        "def Text_summarizer(raw_docx):\n",
        "    raw_text = raw_docx\n",
        "    docx = nlp(raw_text)\n",
        "    stopwords = []  # No need for stopwords in Telugu\n",
        "\n",
        "    word_frequencies = {}\n",
        "    sentences = sentence_tokenize.sentence_split(raw_text, lang='te')\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            if word not in stopwords:\n",
        "                if word not in word_frequencies.keys():\n",
        "                    word_frequencies[word] = 1\n",
        "                else:\n",
        "                    word_frequencies[word] += 1\n",
        "\n",
        "    maximum_frequency = max(word_frequencies.values())\n",
        "\n",
        "    for word in word_frequencies.keys():\n",
        "        word_frequencies[word] = (word_frequencies[word] / maximum_frequency)\n",
        "\n",
        "    # Create vectorizer object\n",
        "    c = CountVectorizer()\n",
        "    bow_matrix = c.fit_transform(sentences)\n",
        "\n",
        "    # Transform count matrix to TF-IDF representation\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    normalized_matrix = tfidf_transformer.fit_transform(bow_matrix)\n",
        "\n",
        "    # Convert to CSR format\n",
        "    normalized_csr_matrix = csr_matrix(normalized_matrix)\n",
        "\n",
        "    # Create networkx graph\n",
        "    nx_graph = nx.Graph()\n",
        "\n",
        "    # Add nodes to the graph\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        nx_graph.add_node(i, sentence=sentence)\n",
        "\n",
        "    # Loop through similarity matrix and add edges\n",
        "    for i in range(normalized_csr_matrix.shape[0]):\n",
        "        for j in range(normalized_csr_matrix.shape[1]):\n",
        "            weight = normalized_csr_matrix[i, j]\n",
        "            if weight > 0:\n",
        "                nx_graph.add_edge(i, j, weight=weight)\n",
        "\n",
        "    # Calculate pagerank scores\n",
        "    scores = nx.pagerank(nx_graph)\n",
        "\n",
        "    # Sort sentences by score\n",
        "    sorted_sentences = sorted(scores, key=scores.get, reverse=True)\n",
        "\n",
        "    # Generate summary\n",
        "    summary = ' '.join([sentences[i] for i in sorted_sentences[:3]])  # Adjust number of sentences in summary as needed\n",
        "    return summary\n",
        "\n",
        "# Define your raw text document\n",
        "raw_text = \"\"\"\n",
        "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. కానీ వెబ్‌సైట్‌లో ఇంటర్నెట్‌తో సంప్రదించిన రైతనే వాటిని విడుదల చేస్తుంది. ఈ ఆఫర్ ఆన్‌లైన్‌లో అందుబాటులోకి రావడానికి ఆరు ఏళ్ల నుంచి రూ.1000 కింద ఖర్చుపెట్టాలి. ఇకపై ప్రస్తుతం మాత్రం రూ.700కే అధికంగా ఖర్చుపెట్టాలి. మీ అకౌంటులో సాధారణంగా డిపాజిట్ ఉన్నప్పుడు, ఈ ఆఫర్‌ను పొందండి. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZkQdxxTQBj",
        "outputId": "eaed26c1-c568-4765-b504-e54d4ac66ef6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ప్రస్తుత సమయంలో, స్పేసి ఒక కృత్రిమ బుద్ధి సంవత్సరానికి సుమారు 10 బిలియన్‌ల పట్టుదల క్రెడిట్ కార్డులను విడుదల చేశాయి. ఇది ఆంధ్రప్రదేశ్‌లో బుద్ధి సంవత్సరానికి బెల్ట్ రోజులును పడించింది. తదితర వివరాలకు వాట్సాప్‌కు రిజిస్టర్ అవ్వండి: 9701146246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"\"\"\n",
        "అనగనగా ఓ రాజు. ఆ రాజు పేరు సుదర్శనుడు. అతనికి చెప్పుకోలేని కష్టం వచ్చి పడింది. అదేమిటంటే... నలుగురు కొడుకులున్నారతనికి. ఆ కొడుకులికి ఆటలంటే ఇష్టం. పాటలంటే ఇష్టం. చదువంటేనే ఇష్టం లేదు. బాగా చదువుకుని, శాస్త్రాలని ఒంటబట్టించుకుంటేనే కదా, గొప్పవారూ, రేపటి రాజులవుతారు. కాని చదువంటేనే ఇష్టం లేదు వాళ్ళకి. అలా అని శుద్ధ మొద్దులా అంటే కాదు, బుద్ధిమంతులే!రాజుగారు ఈ బాధలోనే కొలువు తీరారు. పండితులతోనూ, విద్వాంసులతోనూ రకరకాల చర్చలు చేశారు. ఆ సందర్భంలో ఓ పండితుడు ఇలా అన్నాడు.‘‘మనిషి డబ్బుతోనూ, అధికారంతోనూ, యవ్వనంతోనూ, అవివేకంతోనూ జాగ్రత్తగా ఉండాలి. లేని పక్షంలో ఈ నాలుగింట దేనితోనయినా చెడిపోయే ప్రమాదం ఉంది. కలగలిసి నాలుగూ ఉన్న వాడూ ఇటే ్ట చెడిపోతాడు. అందుకనే మనిషన్నవాడు బాగా చదువుకోవాలి. చదువుకుంటే తెలివితేటలూ. వివేకజ్ఞానం అలవడి, చెడిపోకుండా ఉంటాడు. మనిషికి విద్య కన్నులాంటిది. ఆ కన్ను లేకపోతే కష్టం. బతుకంతా చీకటే’’పండితుని మాటలతో రాజు బాధ రెట్టింపయింది. కొలువు చాలిస్తున్నామని చెప్పి, చరచరా అంతఃపురానికి వెళ్ళిపోయాడు. రాజు, కొలువుని ఇలా మధ్యలో ముగించిన సందర్భాలు లేవు.\n",
        "\"\"\"\n",
        "\n",
        "# Call Text_summarizer function with raw text\n",
        "summary = Text_summarizer(raw_text)\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkatO7XkTl1w",
        "outputId": "f9fff0ed-e09e-4049-985b-3a8ef73132b7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "బతుకంతా చీకటే’’పండితుని మాటలతో రాజు బాధ రెట్టింపయింది. ‘‘మనిషి డబ్బుతోనూ, అధికారంతోనూ, యవ్వనంతోనూ, అవివేకంతోనూ జాగ్రత్తగా ఉండాలి. బాగా చదువుకుని, శాస్త్రాలని ఒంటబట్టించుకుంటేనే కదా, గొప్పవారూ, రేపటి రాజులవుతారు.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EZIexMnUuhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}